{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EndSem_ML.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UIdM7XUwnQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from keras.utils import to_categorical\n",
        "#loading dataset from drive\n",
        "dir_name = '/content/drive/My Drive/ML_Datasets/Mini_Merced/'\n",
        "classes = dict()\n",
        "x = 0\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "#creating dataset\n",
        "for d in os.listdir(dir_name):\n",
        "    print(d)\n",
        "    classes.update({d:x})\n",
        "    x += 1\n",
        "\n",
        "    for img in os.listdir(dir_name+'/'+d):\n",
        "        \n",
        "        im = cv2.imread(dir_name+'/'+d+'/'+img,1)\n",
        "        \n",
        "        im = cv2.resize(im,(224,224),interpolation = cv2.INTER_LINEAR)\n",
        "        X.append(np.array(im))\n",
        "        Y.append(classes[d])\n",
        "\n",
        "print(classes)\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "Y = np.reshape(Y,(-1,1))\n",
        "Y = to_categorical(Y,dtype=int)\n",
        "#saving numpy array of dataset\n",
        "np.save('X.npy',X)\n",
        "np.save('Y.npy',Y)\n",
        "print(X.shape,Y.shape)\n",
        "#print(classes)\n",
        "print(X[:2])\n",
        "print(Y[:2])\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p53USNPzwtXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model,Sequential\n",
        "from keras.layers import Dense, GlobalAveragePooling2D,Dropout\n",
        "from keras import backend as K\n",
        "from keras.datasets import cifar10\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#loading the dataset\n",
        "X = np.load('X.npy')\n",
        "Y = np.load('Y.npy')\n",
        "n_classes = 6\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "train_X = train_X.astype('float32')\n",
        "test_X = test_X.astype('float32')\n",
        "X = X.astype('float32')\n",
        "train_X /= 255\n",
        "X /= 255\n",
        "test_X /= 255\n",
        "\n",
        "\n",
        "print(train_X.shape,test_X.shape,train_Y.shape,test_Y.shape)\n",
        "base_model = VGG16(weights = 'imagenet')\n",
        "base_model.summary()\n",
        "#printing the weights of layer 10 before fine tuning\n",
        "print(base_model.layers[10].get_weights())\n",
        "\n",
        "#setting the first 10 layers as non-trainable\n",
        "for layer in base_model.layers[:5]:\n",
        "    layer.trainable = False\n",
        "    \n",
        "epochs = 2\n",
        "batch_size = 32\n",
        "\n",
        "#creating the model\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(n_classes,activation='softmax'))\n",
        "\n",
        "#model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
        "model.fit(X,Y,epochs = epochs,batch_size = batch_size,validation_data = (test_X,test_Y))\n",
        "scores = model.evaluate(test_X,test_Y,verbose = 0)\n",
        "print(\"Accuracy: %.4f%%\" % (scores[1]*100))\n",
        "\n",
        "#printing the weights after fine tuning\n",
        "print(base_model.layers[10].get_weights())\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKA_QqOKwzO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#saving the features for each input image\n",
        "new_X = []\n",
        "base_model = VGG16(weights = 'imagenet',include_top = False)\n",
        "for x in X:\n",
        "    n_x = np.reshape(x,(1,224,224,3))\n",
        "    new_X.append(base_model.predict_on_batch(n_x))\n",
        "  \n",
        "new_X = np.array(new_X)\n",
        "print(new_X.shape)\n",
        "new_X = np.reshape(new_X,(600,7*7*512))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZZu24x_w2ey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.layers import Dense,Input\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model,Sequential,load_model\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(new_X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "train_X = train_X/255\n",
        "test_X = test_X/255\n",
        "\n",
        "\n",
        "\n",
        "#creating stacked auto-encoder haing 3 auto-encoders\n",
        "\n",
        "input_img = Input(shape=(25088,))\n",
        "encode = Dense(1024,activation='relu')(input_img)\n",
        "output_img = Dense(25088,activation='sigmoid')(encode)\n",
        "encoder_1 = Model(input_img,encode)\n",
        "auto_1 = Model(input_img,output_img)\n",
        "auto_1.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
        "auto_1.fit(train_X, train_X, epochs=5,batch_size=256,validation_data=(test_X,test_X))\n",
        "\n",
        "\n",
        "encoder_1.save('Encoder_1')\n",
        "in_1 = encoder_1.predict(train_X)\n",
        "\n",
        "input_img = Input(shape=(in_1.shape[1],))\n",
        "encode = Dense(512,activation='relu')(input_img)\n",
        "output_img = Dense(in_1.shape[1],activation='sigmoid')(encode)\n",
        "encoder_2 = Model(input_img,encode)\n",
        "auto_2 = Model(input_img,output_img)\n",
        "auto_2.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])  \n",
        "auto_2.fit(in_1, in_1, epochs=5,batch_size=256)\n",
        "\n",
        "encoder_2.save('Encoder_2')\n",
        "in_2 = encoder_2.predict(in_1)\n",
        "\n",
        "input_img = Input(shape=(in_2.shape[1],))\n",
        "encode = Dense(256,activation='relu')(input_img)\n",
        "output_img = Dense(in_2.shape[1],activation='sigmoid')(encode)\n",
        "encoder_3 = Model(input_img,encode)\n",
        "auto_3 = Model(input_img,output_img)\n",
        "auto_3.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])  \n",
        "auto_3.fit(in_2, in_2, epochs=5,batch_size=256)\n",
        "in_3 = encoder_3.predict(in_2)\n",
        "print(in_3.shape)\n",
        "\n",
        "np.save('encoding',in_3)\n",
        "\n",
        "encoder_3.save('Encoder_3')\n",
        "\n",
        "\n",
        "#creating the model of auto-encoders for predicting\n",
        "model = Sequential()\n",
        "x = load_model('Encoder_1')\n",
        "model.add(x)\n",
        "x = load_model('Encoder_2')\n",
        "model.add(x)\n",
        "x = load_model('Encoder_3')\n",
        "model.add(x)\n",
        "\n",
        "model.add(Dense(6,activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])  \n",
        "model.fit(train_X, train_Y, epochs=5,batch_size=128,validation_data = (test_X,test_Y))\n",
        "\n",
        "#predictions and confusion_matrix\n",
        "pred = model.predict(test_X)\n",
        "pred = pred.argmax(axis=1)\n",
        "c_matrix = confusion_matrix(test_Y.argmax(axis=1),pred)\n",
        "print(c_matrix)\n",
        "\n",
        "#accuracy\n",
        "accuracy = accuracy_score(test_Y.argmax(axis=1),pred)\n",
        "print('Accuracy : ',accuracy)\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "#creating svm classifier\n",
        "# clf.fit(in_3, train_Y.argmax(axis=1))\n",
        "clf = OneVsRestClassifier(LinearSVC(random_state=42)).fit(in_3,train_Y.argmax(axis=1) )\n",
        "in_1 = encoder_1.predict(test_X)\n",
        "in_2 = encoder_2.predict(in_1)\n",
        "in_3 = encoder_3.predict(in_2)\n",
        "pred = clf.predict(in_3)\n",
        "\n",
        "#predictions and confusion_matrix  \n",
        "c_matrix = confusion_matrix(test_Y.argmax(axis=1),pred)\n",
        "print(c_matrix)\n",
        "accuracy = accuracy_score(test_Y.argmax(axis=1),pred)\n",
        "print('Accuracy : ',accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}